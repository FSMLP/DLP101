{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "In this notebook, you will have a detailed insight about what a PyTorch Tensor is, different ways to initialize it and use it to perform mathematical operations, store model parameters etc. \n",
    "\n",
    "Tensors are the primary data structure in PyTorch which handles almost every numeric task required in a deep learning workflow. Generally speaking, PyTorch Tensors are multi-dimensional arrays (just like numpy ndarray), with certain specific features like storing gradient values (if required), device interobility (can switch between cpu/gpu), etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are Tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning context, every data structure containing elements of **same** data-type (numeric/boolean) is a tensor. Thus, a tensor can be a scalar value (rank 0 tensor), list/vector (rank 1 tensor), matrix (rank 2 tensor) or any higher order matrix.\n",
    "For example, if you have 1000 color images of 129*128 size, you effective have a rank 4 tensor described as (N, C, H, W);  \n",
    "where, N is the number of samples (1000 here),   \n",
    "       C is the channel size (3 since color),   \n",
    "       H is the height of each image (=128) and    \n",
    "       W is the width of each image (=128).  \n",
    "\n",
    "Tensors in PyTorch are defined in torch.Tensor class which lists out its datatypes, attributes, arguments and methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four ways of creating tensors in PyTorch:\n",
    "\n",
    "1. From GIVEN size description \n",
    "2. Creating Tensor from other Tensor of SAME size\n",
    "3. Creating a Sequence Tensor\n",
    "4. From Numpy Arrays\n",
    "\n",
    "But before creating tensor, there are a couple of things we need to keep in mind.   \n",
    "Firstly, what `dtype` we want our tensors to be in. Default is torch.FloatTensor.\n",
    "Secondly, what `device` we want our tensor to be created on. Default is 'cpu'. \n",
    "\n",
    "Each torch.Tensor has a torch.dtype and torch.device atrribute and almost all tensor creation process gives us the option to specify them.\n",
    "\n",
    "### Data Types in PyTorch\n",
    "\n",
    "PyTorch has specified 12 data types with different variants for 'cpu' and 'gpu'. You probably would never use specific variants for creating tensors, just knowing the dtype would suffice. More information is available here: https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "The default tensor type in PyTorch is `torch.FloatTensor` which is a cpu variant of `torch.float32` dtype. (can be chaged using `torch.set_default_tensor_type()` function).  \n",
    "\n",
    "Some of the common datatype that we may use are:\n",
    "\n",
    "1. torch.float32 / torch.float for 32-bit floating point\n",
    "2. torch.float16 / torch.half for 16-bit floating point\n",
    "3. torch.uint8 for unsigned 8-bit integer (its range is 0-255, generally used in computer vision)\n",
    "4. torch.bool for boolean values\n",
    "\n",
    "For specifying device, you can use, device = 'cpu'/'cuda'/'cuda:0', here 0 represents which cuda device you want to put your tensors on (In case of multi-gpu setups)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Fixed-Size Tensor - Non Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Tensor: \n",
      " tensor([[-6.5921e+31,  4.5719e-41, -6.6178e+31,  4.5719e-41],\n",
      "        [-3.3651e+03, -1.3531e-34, -6.5924e+31,  4.5719e-41],\n",
      "        [-6.6281e+31,  4.5719e-41, -1.2111e+03,  1.9404e-26]]) \n",
      "\n",
      "Zero Filled Tensor: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]) \n",
      "\n",
      "One Filled Tensor: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "Constant Filled Tensor: \n",
      " tensor([[12345, 12345, 12345, 12345],\n",
      "        [12345, 12345, 12345, 12345],\n",
      "        [12345, 12345, 12345, 12345]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating empty tensor\n",
    "empty_tnsr = torch.empty(3,4)\n",
    "\n",
    "# Zero-filled tensor\n",
    "zero_tnsr = torch.zeros(3,4)\n",
    "\n",
    "# One-filled tensor\n",
    "one_tnsr = torch.ones(3,4)\n",
    "\n",
    "# Constant-filled tensor\n",
    "const_tnsr = torch.full((3,4), 12345)\n",
    "\n",
    "print(f'Empty Tensor: \\n {empty_tnsr} \\n\\nZero Filled Tensor: \\n {zero_tnsr} \\n\\nOne Filled Tensor: \\n {one_tnsr} \\n\\nConstant Filled Tensor: \\n {const_tnsr} \\n')\n",
    "\n",
    "# All of them take dtype and device as arguments. For eg:\n",
    "# torch.ones((3,4), device='cuda:0', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Fixed-Size Tensor - Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniformly Distributed Random Tensor: \n",
      " tensor([[0.0028, 0.5947, 0.1142],\n",
      "        [0.7142, 0.8491, 0.5064]]) \n",
      "\n",
      "Uniformly Distributed Random Integer-Filled Tensor: \n",
      " tensor([[2, 2, 3],\n",
      "        [5, 6, 0]]) \n",
      "\n",
      "Normally Distributed Random Tensor: \n",
      " tensor([[-0.1196, -1.8458,  1.6861],\n",
      "        [-0.9417, -1.0843,  0.4500]])\n"
     ]
    }
   ],
   "source": [
    "# Random numbers from uniform distribution on the interval [0,1)\n",
    "unif_tnsr = torch.rand(2,3)\n",
    "\n",
    "# Random integers from uniform distribution on the interval [low,high)\n",
    "low = 0; high = 10\n",
    "unif_int_tnsr = torch.randint(low, high, (2,3))\n",
    "\n",
    "# Random numbers from normal distribution with 0 mean and 1 variance\n",
    "normal_tnsr = torch.randn(2,3)\n",
    "\n",
    "print(f'Uniformly Distributed Random Tensor: \\n {unif_tnsr} \\n\\nUniformly Distributed Random Integer-Filled Tensor: \\n {unif_int_tnsr} \\n\\nNormally Distributed Random Tensor: \\n {normal_tnsr}')\n",
    "\n",
    "# All of them take dtype and device as arguments. For eg:\n",
    "# torch.rand((3,4), device='cuda:0', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Seeding of Random Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True]])\n",
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# For the sake of reproducibility, we fix a seed value at the beginning, so that all generated random tensors in a program are same no matter when and where it is executed.\n",
    "torch.manual_seed(331)\n",
    "a = torch.randint(2,30,(2,5))\n",
    "b = torch.randint(2,30,(2,5))\n",
    "c = torch.rand(2,3)\n",
    "\n",
    "# Lets rerun the scripts using same seed\n",
    "torch.manual_seed(331)\n",
    "d = torch.randint(2,30,(2,5))\n",
    "e = torch.randint(2,30,(2,5))\n",
    "f = torch.rand(2,3)\n",
    "\n",
    "# Check for yourself if a and d are same, b and e are same, c and f are same.\n",
    "print(c==f)\n",
    "print(b==e) \n",
    "\n",
    "print(a==e) # This should not be same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Same-Size Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Like Tensor: \n",
      " tensor([[5.2600e+22, 2.4286e-18, 1.8788e+31, 7.9303e+34, 6.1949e-04],\n",
      "        [7.3313e+22, 7.2151e+22, 2.8404e+29, 2.3089e-12, 7.1856e+22]]) \n",
      "\n",
      "Zero Filled Like Tensor: \n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) \n",
      "\n",
      "One Filled Like Tensor: \n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) \n",
      "\n",
      "Randomly Filled Like Tensor: \n",
      " tensor([[0.1769, 0.4177, 0.8316, 0.4345, 0.6412],\n",
      "        [0.9399, 0.9448, 0.9567, 0.0889, 0.9263]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,5)\n",
    "\n",
    "# empty_like\n",
    "empty_like_tnsr = torch.empty_like(x) \n",
    "\n",
    "# zeros_like\n",
    "zero_like_tnsr = torch.zeros_like(x) \n",
    "\n",
    "# ones_like\n",
    "one_like_tnsr = torch.ones_like(x) \n",
    "\n",
    "# rand_like\n",
    "rand_like_tnsr = torch.rand_like(x) \n",
    "\n",
    "print(f'Empty Like Tensor: \\n {empty_like_tnsr} \\n\\nZero Filled Like Tensor: \\n {zero_like_tnsr} \\n\\nOne Filled Like Tensor: \\n {one_like_tnsr} \\n\\nRandomly Filled Like Tensor: \\n {rand_like_tnsr}')\n",
    "\n",
    "# All of them take dtype and device as arguments. For eg:\n",
    "# torch.zeros_like(x, device='cuda:0', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequence Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arange tensors: \n",
      " tensor([0, 1, 2, 3, 4])\n",
      " tensor([3, 4, 5, 6, 7, 8, 9])\n",
      " tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000])\n",
      "\n",
      "linspace tensor: \n",
      " tensor([ 4.,  8., 12., 16., 20., 24., 28., 32., 36., 40.])\n"
     ]
    }
   ],
   "source": [
    "# arange: return tensor with values between [start (default = 0), end) and step size = 'step' (default 1) \n",
    "arange_tnsr_0 = torch.arange(5)\n",
    "arange_tnsr_1 = torch.arange(3,10)\n",
    "arange_tnsr_2 = torch.arange(1,5,0.5)\n",
    "\n",
    "# linspace: return tensor with values between [start, end) and no of step = 'steps'\n",
    "lrange_tnsr = torch.linspace(4, 40, 10)\n",
    "\n",
    "print(f'arange tensors: \\n {arange_tnsr_0}\\n {arange_tnsr_1}\\n {arange_tnsr_2}\\n\\nlinspace tensor: \\n {lrange_tnsr}')\n",
    "\n",
    "# All of them take dtype and device as arguments. For eg:\n",
    "# torch.arange(5, device='cuda', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From Numpy arrays/ Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3., 5.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.array([[1.,3.,5.]])\n",
    "\n",
    "# torch.from_numpy: Converts numpy arrays (only) to torch tensor. no copy is performed. Thus, changes to the tensor is reflected in numpy array and vice versa\n",
    "from_np_tnsr = torch.from_numpy(array)\n",
    "\n",
    "# Note: from_numpy doesn't take any argument. Thus, adding device/dtype will throw ERROR:\n",
    "# torch.from_numpy(array, device='cpu') # ! will throw ERROR\n",
    "\n",
    "# torch.as_tensor: Converts python lists, numpy arrays, scaler, etc. to torch tensor. copy/no copy depending on whether dtype/device is different/same. \n",
    "as_tnsr0 = torch.as_tensor([1,2])\n",
    "as_tnsr1 = torch.as_tensor(array)\n",
    "\n",
    "# Note: torch.as_tensor takes in dtype and device as arguments. Eg:\n",
    "# torch.as_tensor([1,2], device='cpu', dtype=torch.int8)\n",
    "\n",
    "# Note: Using torch.tensor does same thing as torch.as_tensor except that it will ALWAYS copy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 torch.Tensor Methods: Part 1\n",
    "\n",
    "There several functions and operations that are defined on PyTorch Tensors of which we will look into some of them here in Part 1. Some others tensor methods, mostly related to gradient operations will be discussed in next module as Part 2 of torch.Tensor Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Access and change Tensor Attributes  (`dtype` and `device`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cpu\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "tnsr = torch.rand(3,4)\n",
    "# access datatype and device type\n",
    "print(tnsr.dtype, tnsr.device)\n",
    "\n",
    "# Remember python inbuilt type() function is different from .dtype attribute\n",
    "print(type(tnsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "tensor([[0.3905, 0.1861, 0.8887, 0.0073],\n",
      "        [0.4402, 0.8681, 0.5956, 0.6009],\n",
      "        [0.3325, 0.0368, 0.7867, 0.6241]])\n",
      "\n",
      "Modified\n",
      "tensor([[0.3905, 0.1861, 0.8887, 0.0073],\n",
      "        [0.4402, 0.8681, 0.5956, 0.6009],\n",
      "        [0.3325, 0.0368, 0.7867, 0.6241]], device='cuda:0')\n",
      "tensor([[0.3904, 0.1860, 0.8887, 0.0073],\n",
      "        [0.4402, 0.8682, 0.5957, 0.6011],\n",
      "        [0.3325, 0.0368, 0.7866, 0.6240]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# change datatype and device type\n",
    "\n",
    "# .to: you can change both datatype and device type using this method\n",
    "tnsr.to(device='cuda')\n",
    "# Note it returns a NEW tensor hence needs to be (re)assigned.\n",
    "new_tnsr1 = tnsr.to(device='cuda')\n",
    "\n",
    "# You can also use the same notation to change dtype\n",
    "new_tnsr2 = tnsr.to(dtype=torch.float16)\n",
    "# tnsr.to(device='cpu',dtype=torch.float64)\n",
    "\n",
    "print(f'Original\\n{tnsr}\\n\\nModified\\n{new_tnsr1}\\n{new_tnsr2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ways of changing datatype or device # Not Recommended\n",
    "new_tnsr3 = tnsr.cuda()\n",
    "new_tnsr4 = tnsr.int()\n",
    "new_tnsr5 = tnsr.half().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Create a copy of tensor : `x.clone()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copies the device type, data type and gradient properties of the tensor. We will look into gradient properties in the next module.\n",
    "tnsr_copy = tnsr.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. Convert to Numpy array format: `x.numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x.notebook.stdout": "<class 'numpy.ndarray'>\n"
     },
     "output_type": "unknown"
    },
    {
     "data": {
      "application/x.notebook.error-traceback": {
       "ename": "TypeError",
       "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-110-c7b7d2c236a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# If the tensor has gradient enabled or is not on cpu, it will throw an ERROR.  We will look into gradient properties in the next module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtnsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
       ]
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "np_array = tnsr.cpu().numpy() \n",
    "print(type(np_array))\n",
    "# If the tensor has gradient enabled or is not on cpu, it will throw an ERROR.  We will look into gradient properties in the next module.\n",
    "np_array = tnsr.cuda().numpy() # ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. Change Tensor Shape/Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access tensor shape/size\n",
    "tnsr = torch.rand(3,4)\n",
    "tnsr.shape\n",
    "# tnsr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4163, 0.4046, 0.3389],\n",
      "        [0.8736, 0.1720, 0.5574],\n",
      "        [0.6029, 0.6549, 0.6424],\n",
      "        [0.2368, 0.4377, 0.3623]])\n",
      "tensor([[0.4163, 0.4046, 0.3389],\n",
      "        [0.8736, 0.1720, 0.5574],\n",
      "        [0.6029, 0.6549, 0.6424],\n",
      "        [0.2368, 0.4377, 0.3623]])\n"
     ]
    }
   ],
   "source": [
    "# reshape: Return New Tensor after reshaping it to a different dimension\n",
    "# If one of the dimension is kept -1, it automatically infers that dimension and number of elements\n",
    "new_tnsr = tnsr.reshape(4,-1) # similar to torch.reshape(tnsr, (4,-1))\n",
    "print(new_tnsr)\n",
    "# tnsr.view() also does the same thing but has more specific criteria to be fulfilled. Not Recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.1300, 12.4500]) tensor([[ 3.1300, 12.4500]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1300, 12.4500]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze() = Add a dimension of size one inserted at given position. Useful in case of stretching the single input tensor to include batch dimension (at 0th position).\n",
    "tnsr = torch.as_tensor([3.13,12.45])\n",
    "new_tnsr = tnsr.unsqueeze(0) # similar to torch.unsqueeze(tnsr, 0)\n",
    "print(tnsr, new_tnsr)\n",
    "\n",
    "# unsqueeze_() = Inplace version of unsqueeze()\n",
    "tnsr.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.1300, 12.4500])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squeeze() = Remove all dimensions of size 1 and returns the tensor. \n",
    "new_tnsr = tnsr.squeeze() # similar to torch.unsqueeze(tnsr, 0)\n",
    "print(tnsr.shape, new_tnsr.shape)\n",
    "\n",
    "# squeeze_() = Inplace version of squeeze()\n",
    "tnsr.squeeze_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3e. Perform Mathematical Operations\n",
    "\n",
    "There are in general two ways to perform math ops in PyTorch. One way is to directly invoke the math function on the tensor object itself, for eg: x.sin(), x.greater(y) etc.! The advantage here is that MANY of these math functions also has an associated inplace variant, like x.sin_() or x.greater_(y). The other way is to use torch functions explicitly, like torch.sin(x) or torch.greater(). This method is discussed in section 4.\n",
    "\n",
    "Note: Only a few examples are shown here. More comprehensive list of math ops in torch is listed in section 4. For complete list of math operations on tensor object, refer the official documentaion: https://pytorch.org/docs/stable/tensors.html  "
   ]
  },
  {
   "source": [
    "### In-place operations:\n",
    "\n",
    "In-place operations directly changes the content of a given Tensor without making a copy. \n",
    "PyTorch provides inplace operators as a direct variant to the non inplace one, distinguished by an trailing underscore. For example, sin() function has an inplace variant, sin_(). But note that there are some operations which do not have an inplace variant.\n",
    "\n",
    "The obvious advantage to this is less memory consumption as no new tensor has to be formed. But there is a major caveat to using in-place operations: Gradient values cannot be calculated on in-place operators. Thus, if an operation is within the scope of gradient calculation (or is a part of your model architecture), in-place variants must be avoided."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigonometric ops (sin, cos, tan etc.)\n",
    "new_tnsr = tnsr.sin()\n",
    "tnsr.sin_() # inplace variant\n",
    "\n",
    "# logaritgmic and exponential ops\n",
    "new_tnsr = tnsr.log()\n",
    "tnsr.log_()\n",
    "\n",
    "# comparison ops\n",
    "new_tnsr = tnsr.equal(y)\n",
    "tnsr.equal_(y)\n",
    "\n",
    "# matrix ops\n",
    "new_tnsr = tnsr.inverse()\n",
    "# no inplace variant for inverse\n",
    "\n",
    "# reduction ops\n",
    "new_tnsr = tnsr.mean()\n",
    "# no inplace variant for reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Common Mathematical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Element-wise addition, substraction, multiplication and division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 5.],\n",
      "        [3., 1.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 5.],\n",
      "        [3., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 5.],\n",
      "        [3., 1.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1.,2.],[3.,5.],[3.,1.]])\n",
    "B = torch.zeros(3,2)\n",
    "C = torch.ones(3,2)\n",
    "# Using overloaded operators (+,-,*,/)\n",
    "print(A+B)\n",
    "print(A-B)\n",
    "print(A*B)\n",
    "print(A/C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 5.],\n",
      "        [3., 1.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 5.],\n",
      "        [3., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 5.],\n",
      "        [3., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Using pytorch functions\n",
    "print( torch.add(A,B) ) \n",
    "print( torch.sub(A,B) ) # same as torch.subtract()\n",
    "print( torch.mul(A,B) ) # same as torch.multiply()\n",
    "print( torch.div(A,C) ) # same as torch.divide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Element-wise Transformation (trignometric, exponential, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(A) # same as torch.absolute()\n",
    "torch.ceil(A) # calculates ceiling value\n",
    "torch.floor(A) # calculates floor value\n",
    "torch.reciprocal(A) # returns reciprocal of all elements in a tensor\n",
    "\n",
    "# clips minimum and maximum value of a tensor to min and max respectively\n",
    "torch.clamp(A, min=0.4,max=1.5) # same as torch.clip \n",
    "\n",
    "# trignometric ops\n",
    "torch.sin(A)\n",
    "torch.cos(A)\n",
    "torch.tan(A)\n",
    "torch.sinh(A)\n",
    "torch.cosh(A)\n",
    "torch.tanh(A)\n",
    "\n",
    "# logaritmic and exponential ops\n",
    "torch.log(A)\n",
    "torch.log2(A)\n",
    "torch.log10(A)\n",
    "torch.exp(A)\n",
    "torch.exp2(A)\n",
    "torch.pow(A, 3)\n",
    "torch.sqrt(A);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Element-wise comparison ops (greater, maximum, or, and etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs logical operations (like python and, or , not etc.)\n",
    "torch.logical_or(A,B)\n",
    "torch.logical_not(A)\n",
    "torch.logical_and(A,B)\n",
    "torch.logical_xor(A,B)\n",
    "\n",
    "a = torch.zeros((3,2), dtype=torch.bool)\n",
    "b = torch.ones((3,2), dtype=torch.bool)\n",
    "# performs bitwise operations\n",
    "torch.bitwise_or(a,b)\n",
    "torch.bitwise_not(a)\n",
    "torch.bitwise_and(a,b)\n",
    "torch.bitwise_xor(a,b)\n",
    "\n",
    "# comparison operators\n",
    "torch.equal(A,B)\n",
    "torch.not_equal(A,B)\n",
    "torch.greater(A,B)\n",
    "torch.greater_equal(A,B)\n",
    "torch.less(A,B)\n",
    "torch.less_equal(A,B)\n",
    "torch.maximum(A,B)\n",
    "torch.minimum(A,B);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d. Matrix and Linear Algebra related ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2464, 0.2189],\n",
      "        [0.1544, 0.1686]])\n",
      "tensor([[0.2464, 0.2189],\n",
      "        [0.1544, 0.1686]])\n",
      "tensor([[0.3076, 0.1327],\n",
      "        [0.1311, 0.1741]])\n",
      "tensor([[0.3076, 0.1327],\n",
      "        [0.1311, 0.1741]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(2,2)\n",
    "B = torch.rand(2,2)\n",
    "a = torch.arange(1,6)\n",
    "b = torch.arange(4,9)\n",
    "\n",
    "# matrix multiplication\n",
    "print(A @ B)\n",
    "print(torch.matmul(A,B)) # same as above\n",
    "\n",
    "# matrix transpose\n",
    "print(A.T)\n",
    "print(torch.transpose(A, dim0=0, dim1=1)) # same as above\n",
    "\n",
    "# returns inverse of a square matrix\n",
    "torch.inverse(A)\n",
    "\n",
    "# returns determinant of a square matrix\n",
    "torch.det(A)\n",
    "\n",
    "# eigenvalues and eigenvectors of square matrix\n",
    "torch.eig(A)\n",
    "\n",
    "# dot product of two 1d tensors\n",
    "torch.dot(a,b)\n",
    "\n",
    "# trace of a matrix\n",
    "torch.trace(A)\n",
    "\n",
    "# calculates inner product (dot product in case of 1d tensors)\n",
    "torch.inner(A,B)\n",
    "\n",
    "# calculates outer product of input tensor with a vector\n",
    "torch.outer(a,b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4e. Reduction ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction Operations\n",
    "torch.max(A) # Returns the max value of all elements in A\n",
    "torch.argmax(A) # Returns the indices of the max value of all elements in A\n",
    "torch.min(A) # Returns the min value of all elements in A\n",
    "torch.argmin(A) # Returns the indices of the min value of all elements in A\n",
    "\n",
    "torch.amax(A, dim=1) # Returns the maximum value in the given dimension\n",
    "torch.amin(A, dim=0) # Returns the minimum value in the given dimension\n",
    "\n",
    "# Perform statistical operations \n",
    "torch.mean(A) \n",
    "torch.median(A)\n",
    "torch.mode(A)\n",
    "\n",
    "torch.std(A) # Calculates standard deviation\n",
    "torch.var(A) # Calculates variance\n",
    "torch.norm(A) # Calculates matrix norm/vector norm\n",
    "\n",
    "torch.count_nonzero(A); # Count non zeros in the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4f. Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting\n",
    "torch.sort(A, dim=1) # sorts elements along a given dimension in ascending order \n",
    "torch.msort(A) # sorts elements along its first dimension in ascending order\n",
    "\n",
    "torch.topk(A, k=2, dim=0); # Returns the k largest elements along a given dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Broadcasting of Tensors\n",
    "\n",
    "The goal of broadcasting in tensors is to make them of the same shape so that element wise operation can be performed on them. For example, multiplying `[2]` with `[3,4]` will yield `[6,8]` as a result of broadcasting, even when their dimensions are not equal. \n",
    "\n",
    "If the tensors satisfy the rules of broadcasting, then it is automatically applied to the respective tensor. Rules for broadcasting are as follows:\n",
    "\n",
    "1. Tensors must have atleast one dimension.   \n",
    "2. Tensor dimensions must be compatible with each other, i.e., traverse from Last to First dimension, and for EACH dim 'i', check if:   \n",
    "    a. dim 'i' of BOTH tensors are equal, OR  \n",
    "    b. dim 'i' of either one of tensors = 1, OR   \n",
    "    c. dim 'i' doesn't exist in one of the tensor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5760, 0.3414, 0.1601, 0.9438],\n",
      "        [0.3104, 0.2757, 0.2393, 0.5958],\n",
      "        [0.8138, 0.3213, 0.0454, 0.8992]])\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "A = torch.rand(3,4)\n",
    "B = torch.rand(1,4)\n",
    "\n",
    "print( A * B) # Here dim=2 is same (2a satisfied) and dim=1 is 1 in B (2b satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0947, 0.2014],\n",
      "        [0.1436, 0.0672],\n",
      "        [0.1100, 0.2458]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "A = torch.rand(3,2)\n",
    "B = torch.rand(  2)\n",
    "\n",
    "print( A * B) # Here dim=2 is same (2a satisfied) and dim=1 doesn't in B (2c satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0351, 0.1868, 0.0954, 0.0715],\n",
      "          [0.1261, 0.6710, 0.3429, 0.2569],\n",
      "          [0.0653, 0.3474, 0.1775, 0.1330],\n",
      "          [0.0456, 0.2428, 0.1240, 0.0929]],\n",
      "\n",
      "         [[0.0118, 0.0630, 0.0322, 0.0241],\n",
      "          [0.0831, 0.4421, 0.2259, 0.1692],\n",
      "          [0.0386, 0.2055, 0.1050, 0.0786],\n",
      "          [0.0377, 0.2007, 0.1025, 0.0768]]],\n",
      "\n",
      "\n",
      "        [[[0.1192, 0.6344, 0.3241, 0.2428],\n",
      "          [0.0313, 0.1667, 0.0852, 0.0638],\n",
      "          [0.1501, 0.7987, 0.4081, 0.3057],\n",
      "          [0.0239, 0.1271, 0.0649, 0.0486]],\n",
      "\n",
      "         [[0.1508, 0.8026, 0.4101, 0.3072],\n",
      "          [0.0341, 0.1813, 0.0927, 0.0694],\n",
      "          [0.0175, 0.0934, 0.0477, 0.0357],\n",
      "          [0.0660, 0.3512, 0.1795, 0.1344]]],\n",
      "\n",
      "\n",
      "        [[[0.0680, 0.3620, 0.1849, 0.1386],\n",
      "          [0.1637, 0.8714, 0.4452, 0.3336],\n",
      "          [0.1423, 0.7572, 0.3869, 0.2899],\n",
      "          [0.1594, 0.8487, 0.4336, 0.3249]],\n",
      "\n",
      "         [[0.0545, 0.2900, 0.1482, 0.1110],\n",
      "          [0.0615, 0.3273, 0.1673, 0.1253],\n",
      "          [0.1669, 0.8886, 0.4541, 0.3402],\n",
      "          [0.1501, 0.7990, 0.4082, 0.3058]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "A = torch.rand(3,2,4,1)\n",
    "B = torch.rand(  1,4  )\n",
    "\n",
    "print( A * B) # Here dim=4 doesn't exist in B, dim=3 is same, dim=2 is 1 in B & dim=1 doesn't in B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b0ae747aab2fd41c38b6ddf5d3cc6f6efaa8ed1a54eca9c93a1701a79020209"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}